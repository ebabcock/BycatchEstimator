<!DOCTYPE html>
<!-- Generated by pkgdown: do not edit by hand --><html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="description" content="BycatchEstimator">
<title>Bycatch-Estimator User Guide • BycatchEstimator</title>
<script src="../deps/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<link href="../deps/bootstrap-5.1.3/bootstrap.min.css" rel="stylesheet">
<script src="../deps/bootstrap-5.1.3/bootstrap.bundle.min.js"></script><!-- Font Awesome icons --><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/all.min.css" integrity="sha256-mmgLkCYLUQbXn0B1SRqzHar6dCnv9oZFPEC1g1cwlkk=" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/v4-shims.min.css" integrity="sha256-wZjR52fzng1pJHwx4aV2AO3yyTOXrcDW7jBpJtTwVxw=" crossorigin="anonymous">
<!-- bootstrap-toc --><script src="https://cdn.rawgit.com/afeld/bootstrap-toc/v1.0.1/dist/bootstrap-toc.min.js"></script><!-- headroom.js --><script src="https://cdnjs.cloudflare.com/ajax/libs/headroom/0.11.0/headroom.min.js" integrity="sha256-AsUX4SJE1+yuDu5+mAVzJbuYNPHj/WroHuZ8Ir/CkE0=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/headroom/0.11.0/jQuery.headroom.min.js" integrity="sha256-ZX/yNShbjqsohH1k95liqY9Gd8uOiE1S4vZc+9KQ1K4=" crossorigin="anonymous"></script><!-- clipboard.js --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script><!-- search --><script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/6.4.6/fuse.js" integrity="sha512-zv6Ywkjyktsohkbp9bb45V6tEMoWhzFzXis+LrMehmJZZSys19Yxf1dopHx7WzIKxr5tK2dVcYmaCk2uqdjF4A==" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/autocomplete.js/0.38.0/autocomplete.jquery.min.js" integrity="sha512-GU9ayf+66Xx2TmpxqJpliWbT5PiGYxpaG8rfnBEk1LL8l1KGkRShhngwdXK1UgqhAzWpZHSiYPc09/NwDQIGyg==" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/mark.min.js" integrity="sha512-5CYOlHXGh6QpOFA/TeTylKLWfB3ftPsde7AnmhuitiTX4K5SqCLBeKro6sPS8ilsz1Q4NRx3v8Ko2IBiszzdww==" crossorigin="anonymous"></script><!-- pkgdown --><script src="../pkgdown.js"></script><meta property="og:title" content="Bycatch-Estimator User Guide">
<meta property="og:description" content="BycatchEstimator">
<!-- mathjax --><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js" integrity="sha256-nvJJv9wWKEm88qvoQl9ekL2J+k/RWIsaSScxxlsrv8k=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/config/TeX-AMS-MML_HTMLorMML.js" integrity="sha256-84DKXVJXs0/F8OTMzX4UR909+jtl4G7SPypPavF+GfA=" crossorigin="anonymous"></script><!--[if lt IE 9]>
<script src="https://oss.maxcdn.com/html5shiv/3.7.3/html5shiv.min.js"></script>
<script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
<![endif]-->
</head>
<body>
    <a href="#main" class="visually-hidden-focusable">Skip to contents</a>
    

    <nav class="navbar fixed-top navbar-dark navbar-expand-lg bg-primary"><div class="container">
    
    <a class="navbar-brand me-2" href="../index.html">BycatchEstimator</a>

    <small class="nav-text text-muted me-auto" data-bs-toggle="tooltip" data-bs-placement="bottom" title="">0.0.0.9000</small>

    
    <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbar" aria-controls="navbar" aria-expanded="false" aria-label="Toggle navigation">
      <span class="navbar-toggler-icon"></span>
    </button>

    <div id="navbar" class="collapse navbar-collapse ms-3">
      <ul class="navbar-nav me-auto">
<li class="nav-item">
  <a class="nav-link" href="../reference/index.html">Reference</a>
</li>
<li class="active nav-item dropdown">
  <a href="#" class="nav-link dropdown-toggle" data-bs-toggle="dropdown" role="button" aria-expanded="false" aria-haspopup="true" id="dropdown-articles">Articles</a>
  <div class="dropdown-menu" aria-labelledby="dropdown-articles">
    <a class="dropdown-item" href="../articles/UserGuide2.html">Bycatch-Estimator User Guide</a>
  </div>
</li>
      </ul>
<form class="form-inline my-2 my-lg-0" role="search">
        <input type="search" class="form-control me-sm-2" aria-label="Toggle navigation" name="search-input" data-search-index="../search.json" id="search-input" placeholder="Search for" autocomplete="off">
</form>

      <ul class="navbar-nav"></ul>
</div>

    
  </div>
</nav><div class="container template-article">



<script src="UserGuide2_files/accessible-code-block-0.0.1/empty-anchor.js"></script><div class="row">
  <main id="main" class="col-md-9"><div class="page-header">
      <img src="" class="logo" alt=""><h1>Bycatch-Estimator User Guide</h1>
                        <h4 data-toc-skip class="author">Elizabeth A. Babcock</h4>
            
            <h4 data-toc-skip class="date">2022-09-14</h4>
      
      
      <div class="d-none name"><code>UserGuide2.Rmd</code></div>
    </div>

    
    

<p><a href="mailto:ebabcock@rsmas.miami.edu" class="email">ebabcock@rsmas.miami.edu</a></p>
<div class="section level2">
<h2 id="introduction">Introduction<a class="anchor" aria-label="anchor" href="#introduction"></a>
</h2>
<p>The BycatchEstimator tool runs a generic model-based bycatch estimation procedure, after you first set up the data inputs and other specifications. The code can estimate both total bycatch, calculated by expanding a sample, such as an observer database, to total effort from logbooks or landings records, and an annual index of abundance, calculated only from the observer data.</p>
<p>The code runs best in RStudio. Before running the code for the first time, install the latest versions of R (<span class="citation">R Core Team (2020)</span>) and RStudio (<span class="citation">RStudio Team (2020)</span>). The following libraries are used: tidyverse, ggplot2, MASS, lme4, cplm, tweedie, DHARMa, tidyselect, MuMIn, gridExtra, pdftools, foreach, doParallel, reshape2, glmmTMB and quantreg (<span class="citation">Wickham et al. (2019)</span>; <span class="citation">Wickham (2016)</span>; <span class="citation">Venables and Ripley (2002)</span>; <span class="citation">Bates et al. (2015)</span>; <span class="citation">Zhang (2013)</span>; <span class="citation">Dunn and Smyth (2005)</span>; <span class="citation">Hartig (2020)</span>; <span class="citation">Henry and Wickham (2020)</span>; <span class="citation">Barton (2020)</span>; <span class="citation">Auguie (2017)</span>; <span class="citation">Wickham and Pedersen (2019)</span>; <span class="citation">Iannone, Cheng, and Schloerke (2020)</span>; <span class="citation">Ooms (2020)</span>;<span class="citation">Microsoft and Weston (2020)</span>;<span class="citation">Corporation and Weston (2020)</span>; <span class="citation">Wickham (2007)</span>; <span class="citation">Brooks et al. (2017)</span>; <span class="citation">Koenker (2021)</span>). The output figures and tables are printed to a pdf file using R Markdown and the knitr library, which outputs a LaTex file; therefore, you must have a LaTex program installed, such as TinyTex (<span class="citation">Xie (2019)</span>, <span class="citation">Xie (2021)</span>, <span class="citation">Xie (2015)</span>).</p>
<p>The R code estimates total bycatch as follows. First, mean catch per unit effort (CPUE) of observed sample units (trips in this example, but it could be sets) is estimated from a linear model with predictor variables. The observation error models used are delta-lognormal, delta-gamma, negative binomial (from either glm.nb in the MASS library or glmmTMB, nbinom1 and nbinom2) and Tweedie (from cpglm or glmmTMB). For comparison, simple normal and lognormal models are also available, although these methods are not expected to perform well with many zero bycatch observations. Within each observation error model group, potential predictor variables are chosen based on the user’s choice of information criteria (AICc, AIC or BIC) (<span class="citation">Barton (2020)</span>). The user specifies a most complex and simplest model, and all intermediate models are considered. The user-specified simplest model will usually include year, and can also include, for example, stratification variables that are used in the observer program sampling design. The model with the lowest value of the information criterion is chosen as best within each observation error group.</p>
<p>The best candidate models in each observation error group are then compared using 10-fold cross validation, if desired, to see which observation error model best predicts CPUE. Note that information criteria can not be used directly to compare, for example, delta-lognormal to negative binomial or Tweedie, because the observation error models have different y data. However, the models can be used to predict CPUE directly, and these predictions can be compared with cross validation. The best model according to cross validation is the one with the lowest root mean square error (RMSE) in the predicted CPUE and the mean error (ME) closest to zero, excluding from consideration models that do not fit well according to criteria described below. Note that this model selection using information criteria and cross validation is only intended as a guide. The user should also look at the information criteria across multiple models, as well as residuals and other diagnostics, and may want to choose a different model for bycatch estimation or abundance index calculation based on other criteria, such as the design of the observer sampling program.</p>
<p>For the best model in each observation error model group (according to the selected information criterion), the total bycatch is estimated by predicting the catch in all logbook trips (i.e., the whole fishery) from the fitted model and summing across trips. There is an option to only predict bycatch from unobserved effort (i.e. trips or sets not sampled by observers) and calculate total bycatch as the observed bycatch plus the predicted bycatch in unobserved effort. This only works if it is possible to match the observed trips to the logbook trips, and the amount of observed effort in a trip is always less than or equal to the amount of total effort. For fisheries with high observer coverage (e.g. 20% or more), predicting bycatch from only unobserved effort would be preferred, because treating the whole fishery as unobserved might overestimate the variance.</p>
<p>The catch in each trip is predicted directly by the negative binomial models. Tweedie and normal models predict CPUE, which is then multiplied by effort. Delta-lognormal and delta-gamma models have separate components for the probability of a positive CPUE and the CPUE, which must be multiplied together (with appropriate bias corrections) and multiplied by effort to get the total catch. Catch in each trip is summed across trips to get the total catch in each year. The variance of the prediction in each trip is calculated as the variance of the prediction interval, which is the variance of the estimated mean prediction plus the residual variance. Because the predicted catches in each logbook trip are dependent on linear model coefficients, which are the same across multiple trips, the trips are not independent; thus, the variances cannot be added without accounting for the covariance. The variance of the total catch in each year is thus calculated either using Monte Carlo simulation, or using a delta method. Users may also chose not to estimate variances for large logbook datasets where these methods will not work. In the case where only the unobserved bycatch is estimated, the observed bycatch is added to the predictions as a known constant with no variance.</p>
<p>For the Monte Carlo variance estimation method, we first draw random values of the linear model coefficients from a multivariate normal distribution with the mean and variance/covariance matrix estimated from the model. The predictions for each trip are then drawn for each draw of the parameters using the appropriate probability density function (e.g. Tweedie, negative binomial) with additional parameters (e.g. residual variance, negative binomial dispersion, Tweedie power and phi) estimated by the model. Trips are then summed for each year (adding the observed catch if necessary) within each draw, and the mean, standard error, and quantiles (e.g. 2.5% and 97.5% for a 95% confidence interval) are then calculated across the Monte Carlo draws. An approximation of the total variance of the predicted bycatch can also be made using a delta method. The delta method approximates the variance of a function of a variable as the derivative of the function squared times the variance of the original variable. Thus, the variance of the prediction intervals in the original data scale is calculated by pre and post multiplying the derivative of the inverse link function to the variance covariance matrix of the predicted values.</p>
<p><span class="math display">\[\Sigma_p = J \Sigma_l J'  \]</span></p>
<p>were <span class="math inline">\(\Sigma_l\)</span> is the variance covariance matrix for the predicted trips in the stratum on the scale of the log link, and J is the matrix of derivatives. See the function MakePredictionsDeltaVar in bycatchFunctions.R for the details for each model type. This code is partly based on the method developed by <a href="https://stackoverflow.com/questions/39337862/linear-model-with-lm-how-to-get-prediction-variance-of-sum-of-predicted-value" class="external-link uri">https://stackoverflow.com/questions/39337862/linear-model-with-lm-how-to-get-prediction-variance-of-sum-of-predicted-value</a>. The delta method is not available for delta-gamma, delta-lognormal, or Tweedie via the cplm library. For those error models, the simulation method will be used even if the delta method is selected with VarCalc.</p>
<p>If the logbook data is aggregated across multiple trips (e.g. by strata) the effort is allocated equally to all the trips in a row of the logbook data table for the purpose of simulating catches or estimating variances using the delta method. This allocation procedure is not needed to estimate the mean total bycatch, but it is necessary to estimate the variances correctly. When using aggregated effort, it is not yet possible to include the observed catches as known.</p>
<p>The model can estimate bycatch and abundance indices for multiple species or dispositions (e.g. dead discard, live release) from the same fishery simultaneously if they are all being estimated from the same data sets.</p>
<p>If a user requests an annual abundance index, this is calculated from the same models as were selected for bycatch estimation, with an option to specify the simplest model to be considered.</p>
</div>
<div class="section level2">
<h2 id="data-specification">Data specification<a class="anchor" aria-label="anchor" href="#data-specification"></a>
</h2>
<p>This provides guidance and summarizes arguments of the function <code>bycatchSetup</code></p>
<p>The observer data should be aggregated to the appropriate sample unit, either trips or sets. Effort must be in the same units in both data sets (e.g. sets or hook-hours). The logbook data may be aggregated to sample units, or it may be aggregated further, as long as it includes data on all stratification or predictor variables. For example, the data can be aggregated by year, region and season if those are the stratification variables, or it can be aggregated by trip. If any environmental variables, such as depth, are included, the logbook data probably has to be entered at the set level. The observer data should have columns for year and the other predictor variables, the observed effort and the observed bycatch or catch per trip of each species to be estimated. The logbook data must also have year and the other predictor variables, and the total effort in the same units (e.g. sets or hook-hours) as the observer data. There should also be a column that reports how many sample units (i.e. trips) are included in each row in the logbook data if the data are aggregated. This is needed to predict catches by trip for the variance calculations. If there are any NA values in any of the variables, those rows will be deleted from the data set. To include the observed catches as known in the totals, it is necessary to include a column for unsampled effort in the logbook data (which can be zero in completely sampled trips) and a column in both the logbook and observer data that can be used to match observed trips to logbook records.</p>
<p>Next, give the names of the variables in the observer and (if estimating bycatch) logbook data files. You must also specify the common and scientific names of the species being analyzed, the units of the bycatch estimates (e.g. kg, numbers), and the type of catch (e.g. retained catch, dead discards, live releases). If analyzing more than one species or disposition type, some of these inputs must be vectors with the same length as the number of species and/or disposition types. If calculating catch as the sum of observed catch and predicted catch from unobserved effort (includeObsCatch=TRUE) then you must have columns for both total effort and unsampled effort in the logbook data, and a column to match observed trips to the logbook trips.</p>
<p>Give the formulas for the most complex and simplest model to be considered. If the simplest model requires stratification variables other than year, summaries of the predicted bycatch at the level of these stratification variables will be printed to .csv files, but will not be plotted automatically. Abundance indices will be calculated including all the variables requested in indexVars, to allow for different indices for different stratification variables if desired (e.g. different spatial areas). You may also specify which predictor variables should be interpreted as factor (categorical). The named variables will be converted to factor before analysis. Note that year may be either a number or a factor. If year is a number, then a model such as <em>y ~ Year</em> will estimate a linear trend across years. Polynomial regression may be a useful way to estimate more complex trends across years in data sets where not all years have enough data to estimate Year as a categorical fixed effect. This can be specified as, for example <em>y ~ Year + I(Year^2) + I(Year^3).</em></p>
</div>
<div class="section level2">
<h2 id="model-specification">Model specification<a class="anchor" aria-label="anchor" href="#model-specification"></a>
</h2>
<p>This provides guidance and summarizes arguments of the function <code>bycatchFit</code></p>
<p>First, specify which models to try and which information criterion to use in narrowing down the predictor variables to use in each observation error model group. Model selection is done with dredge function in the MuMIn library(<span class="citation">Barton (2020)</span>). Note that the tweedie outputs should be the same whether using TMB (TweedieTMB) or the cpglm function (Tweedie). The negative binomial 2 in TMB is the same as the negative binomial in glm.nb. To get faster results, use TMB only for these distributions. They are both included for comparison.</p>
<p>Also, specify whether to do cross validation to choose between observation error models, and whether to use the dredge function to use information criteria to choose the best set of predictor variables for each fold in cross validation. If DredgeCrossValidation is FALSE, the same predictor variables will be used for each fold, as selected for the full data set. This saves time with large data sets. The variable ResidualsTest allows excluding from cross validation any model where the residuals have a P&lt;0.01 for a Kolmogorov Smirnov test of whether the residuals are distributed as expected under the likelihood (testUniformity in DHARMa). This is useful for excluding poorly performing models. However, it may be too restrictive if none of the models perform well, and it only works for small datasets. Specify the confidence levels desired for the total bycatch calculations.</p>
<p>If the variable useParallel is true and your computer has multiple cores, the dredge function will be run in parallel. This greatly speeds up the calculations. If you have trouble getting this to work, set useParallel to FALSE.</p>
<p>Finally, if you have information on total bycatch in each year to validate your estimates, for example in a simulation study, fill out the arguments <code>plotValidation</code>, <code>trueVals</code>, <code>trueCols</code>. Otherwise, set these arguments to FALSE, NULL and NULL, respectively. To include validation data, <code>trueVals</code> should be set equal to a character string containing a filename (with complete path) for a file containing a column labelled “Year”, and columns with the total bycatch in each year, with names specified in <code>trueCols</code>. For multiple species, <code>trueCols</code> can be a vector giving the names of all the column for each species in order.</p>
</div>
<div class="section level2">
<h2 id="reviewing-the-model-setup">Reviewing the model setup<a class="anchor" aria-label="anchor" href="#reviewing-the-model-setup"></a>
</h2>
<p>After running the function <code>bycatchSetup</code>, data summaries are output to directories named “output” followed by the specified run name. A pdf file with summaries for all species (DataSummary.pdf) is placed in the main output folder, and a csv file for each species is placed in an output file named for the species. Note that records with NA in the catch or effort variable are excluded from the analysis and not included in the estimated sample size. If there are any years with no data, or no positive observations, you may want to exclude those years from the analysis. The summary table also counts the number of outliers (defined as data points more than 8 standard deviations from the mean) because outliers cause problems with fitting in some models. For data-checking, this output file also includes columns with estimated bycatch and its variance using a simple unstratified ratio estimator by year (See BycatchFunctions.r for all the functions). You will get an error message if any of the variables in the specified models are not found in the data frame.</p>
</div>
<div class="section level2">
<h2 id="main-analysis">Main analysis<a class="anchor" aria-label="anchor" href="#main-analysis"></a>
</h2>
<p>This section explains the use of the function <code>bycatchFit</code> to run the CPUE models, estimate total bycatch and/or abundance indices, and do cross validation if requested. You can ignore the warnings and information about the models as long as the code keeps running. Most of these are information about which variables are being tried and which models have converged, which will be summarized in the output files.</p>
<p>The model may be slow if you have a large data set or are doing cross validation. The outputs all go into the directories labeled with the species names, and include a pdf of all results, and separate csv files for all the tables. When the models are fit, the code keeps track of whether the model converged correctly, or if not, where it went wrong. An output table called modelFail.csv summarizes the results, with a “-” for models that converged successfully, “data” for models that could not be fit due to insufficient data (no positive observations in some year prevents fitting the delta models), “fit” for models that failed to converge, “cv” for models that produced results with unreasonably high CVs (&gt;10) in the annual catch predictions, and “resid” for models that failed the Kolmogorov Smirnoff test for having the correct distribution in the DHARMa library (<span class="citation">Hartig (2020)</span>), if <code>residualTest</code> is TRUE. Models that fail in any of these ways are discarded and not used in cross validation. If you get one of these errors for a model you want to use, you should check the data for missing combinations of predictor variables, extreme outliers, or years with too few positive observations.</p>
<div class="section level4">
<h4 id="finding-best-approximating-models">Finding best approximating models<a class="anchor" aria-label="anchor" href="#finding-best-approximating-models"></a>
</h4>
<p>The first section of the code includes two loops that go through all the models in <code>modelTry</code> and find the best model using the specified information criterion. The first loop runs all the models that are applied to all data together (i.e. not delta models). The delta models are run separately in the second loop because they require a different data setup. There is a function called findBestModelFunc, which applies the dredge function from the MuMin library to find the best combination of the predictor variables according to the specified information criteria (MuMIn). The dredge function produces a table which includes all the information criteria for each model that was considered, as well as model weights calculated for the information criterion the user specified, which sum to one and indicate the degree of support for the model in the data. The best model will have the highest weight. But, in some cases other models with also have strong support, and should perhaps be considered, particularly if they are simpler. The current version of the code does not use MuMIn’s model averaging function, but this may be worth considering if several models have similar weights.</p>
<p>A binomial model will be tried if it was requested, or if either a delta-lognormal or delta-gamma model were requested, since delta models have a binomial component. The binomial models are fitted using the glm function with a logit link. The negative binomial is run using the glm.nb function from the MASS library (<span class="citation">Venables and Ripley (2002)</span>) or nbinom1 nbinom2 from the glmmTMB library(<span class="citation">Brooks et al. (2017)</span>). The glm.nb function is very similar to the nbinom2 method in glmTMB, but both are included for comparison. Both define the variance of the negative binomial as: <span class="math display">\[\sigma^2=\mu+\mu^2/\theta\]</span>, where <span class="math inline">\(\theta\)</span> is an estimated parameter. For nbinom1, the variance is defined as: <span class="math display">\[\sigma^2= \mu(1+\alpha)\]</span>, where <span class="math inline">\(\alpha\)</span> is an estimated parameter. This version of the negative binomial model, which is equivalent to a quasi-Poisson model, gives somewhat different results from the other negative binomial models.</p>
<p>The negative binomial predicts integer counts, so it is appropriate for predicting bycatch in numbers per trip for all the trips in the logbook data. To allow this model to also be used with catch or bycatch measured in weights, the code rounds the catches to integers before running this model. Check that this is appropriate for the units you are using. To predict CPUE it is necessary to include an offset in the model. We use a log link for all three negative binomial models, so that the model predicts: <span class="math display">\[log(C_i)=b_0+ b_1x_1+offset(log(E_i))\]</span> were <span class="math inline">\(C_i\)</span> is the catch in trip <span class="math inline">\(i\)</span> in the observer data, <span class="math inline">\(b_0+ b_1x_1\)</span> is an example linear predictor with an intercept and a slope, and the offset is the log of the effort <span class="math inline">\(E_i\)</span> in each trip. This is algebraically equivalent to modeling CPUE as a function of the same linear predictor (without the offset).</p>
<p>The Tweedie distribution is available using the cpglm function in the cplm library (<span class="citation">Zhang (2013)</span>) or the Tweedie family in glmmTMB (<span class="citation">Brooks et al. (2017)</span>) by including “Tweedie” or “TMBtweedie” in modelTry. The Tweedie is a generalized function that estimates a distribution similar to a gamma distribution, except that it allows extra probability mass at zero. It is thus appropriate for continuous data with extra zeros. It uses a log link, and, in addition to the linear predictor for the log(mean) it estimates an index parameter <span class="math inline">\(p\)</span> and dispersion parameter <span class="math inline">\(\phi\)</span> which together determine the shape of the distribution.</p>
<p>If all years have at least one positive observation, the code next runs the lognormal and gamma models if requested. For the delta lognormal model, the CPUE is log transformed, and the mean CPUE for positive observations is modeled with the lm function for positive data only. For the delta-gamma method, the log link is used to model the positive CPUE values, using the glm function. The lognormal and gamma method are similar, except that they are run on all the CPUE data, including zeros, after adding a constant of 0.1.</p>
</div>
<div class="section level4">
<h4 id="model-residuals-and-residual-diagnostics">Model residuals and residual diagnostics<a class="anchor" aria-label="anchor" href="#model-residuals-and-residual-diagnostics"></a>
</h4>
<p>The next section loops through all the models again, and calculates the model residuals, residual diagnostics from the DHARMa library (<span class="citation">Hartig (2020)</span>), and, if requested, calculations of the total bycatch and an index of abundance.</p>
<p>For the best model (according to the information criterion) in each observation error model group, both ordinary residuals and DHARMa scaled residuals are plotted, and the DHARMa diagnostics are calculated. The DHARMa library uses simulation to generate scaled residuals based on the specified observation error model so that the results are more clearly interpretable than ordinary residuals for non-normal models. DHARMA draws random predicted values from the fitted model to generate an empirical predictive density for each data point and then calculates the fraction of the empirical density that is greater than the true data point. Values of 0.5 are expected, and values near 0 or 1 indicate a mismatch between the data and the model. Particularly for the binomial and negative binomial models, in which the ordinary residuals are not normally distributed, the DHARMa residuals are a better representation of whether the data are consistent with the assumed distribution. Both the regular residuals and the DHARMa residuals are appropriate for lognormal and gamma models, since they model continuous data which is expected to be approximately normal when transformed by the link function. The DHARMA residuals should be uniformly distributed, as indicated by the QQUniform plot and the Kolmogorov-Smirnov test of uniformity. If the DHARMA residuals show significant over-dispersion then the model is not appropriate. A summary of the the DHARMa residual diagnostics is added to a table called residualTab, and models that fail the Kolmogorov-Smirnov test for uniformity of the DHARMa scaled residuals are excluded from cross validation if <code>residualTest</code> is TRUE. Because the cpglm functions do not produce estimates of standard error, simulation is used to generate the DHARMa residuals if “Tweedie” is the model selected, using the rtweedie distribution from the tweedie library (<span class="citation">Dunn and Smyth (2005)</span>).</p>
</div>
<div class="section level4">
<h4 id="bycatch-and-abundance-index-calculation">Bycatch and abundance index calculation<a class="anchor" aria-label="anchor" href="#bycatch-and-abundance-index-calculation"></a>
</h4>
<p>For each model group the best model, as selected by the information criteria, is used to predict the mean and variance of the total bycatch in each year with the exception of the binomial, for which the model predicts to the total number of positive trips. For the binomial model, the best model is used to predict probability of a positive observation in each logbook trip and these are summed to get the total number of positive trips in each year (and in each stratum if further stratification was requested). The number of positive trips is calculated because, for a very rare species that is never caught more than once in a trip, the number of positive trips would be a good estimate of total bycatch and many of the other models would fail to converge. For more common species, the estimates of total catch are more appropriate, so the results of the binomial model alone are not included in the cross validation for model comparison. An abundance index based on the binomial distribution is also calculated, if requested.</p>
<p>For all the other model types, the functions <code>makePredictionSimVar</code>, <code>makePredictionDeltaVar</code> and <code>makePredictionNoVar</code> (depending on the selected variance calculation method) calculate the total bycatch in each year, by first predicting the total catch in each trip in the logbook data and then summing over all trips in each stratum. For all the negative binomial models, the log(effort) from the logbook trips is used as an offset in the predictions, along with the values of all the predictor variables, so that the model can predict bycatch in each trip directly. For the Tweedie, normal, and gamma models the CPUE is predicted for each trip in the logbook data and must be multiplied by effort and summed across trips to get the annual summaries. For delta-gamma models, the predicted bycatch in each trip is the predicted probability of a positive observation from the binomial, times the predicted CPUE from the gamma model, times effort in the trip. For delta-lognormal models, the variance of the predicted CPUE is needed to bias-correct when converting the mean predicted log(CPUE) to mean predicted CPUE. The variance of the prediction interval for each trip is calculated as the variance of the estimated mean plus the residual variance, and this value is used in the bias correction. The total predicted CPUE is the predicted probability of a positive observation from the binomial times the predicted positive CPUE, and predicted catch is the predicted CPUE times effort. For all models, the predicted total catches are summed across trips to get the totals in each stratum. The variance are calculated using the Monte Carlo simulation method described above or using a delta method. The delta method variance is not available for the delta-lognormal, delta-gamma or Tweedie models using cplm, although it is available for Tweedie using glmmTMB.</p>
<p>If a user requests an annual abundance index, this is also calculated from the best model in each model group. The annual abundance index is calculated by setting all variables other than year, and any variables required to be included in the index (e.g. region or fleet) to a reference level, which is the mean for numerical variables or the most common value for categorical variables. The index is calculated by predicting the mean CPUE in each year, and its standard error is calculated as the standard error of the mean prediction. For delta-lognormal and delta-gamma models the standard error of the prediction is calculated from the means and standard errors of the binomial and positive catch models using the method of <span class="citation">Lo, Jacobson, and Squire (1992)</span>.</p>
<p>Finally, this section combines all the bycatch estimates and index estimates into data frames, and prints out CSV files with the DHARMa residual diagnostics. These are P values for a Kolmogorov-Smirnoff test of whether the DHARMa residuals are uniformly distributed as expected, a test of over-dispersion, a test of zero-inflation (which is meaningless for the delta models, but helpful to see if the negative binomial and tweedie models adequately model the zeros) and a test of whether there are more outliers than expected.</p>
</div>
<div class="section level4">
<h4 id="cross-validation">Cross validation<a class="anchor" aria-label="anchor" href="#cross-validation"></a>
</h4>
<p>The next part runs the cross validation if requested. Only observation error models that converged and produced reasonable results with the complete data set are used in cross validation. For example, if there were not enough positive observations in all years to estimate delta-lognormal and delta-gamma models, then they will not be included in the cross validation.</p>
<p>For cross validation, the observer data are randomly divided into 10 folds. Each fold is left out one at a time and the models are fit to the other 9 folds. The same procedure described above is used to find the best model within each observation error group using information criteria and the MuMIn library if <code>DredgeCrossValidation</code> is TRUE; otherwise, the same variables will be used in each fold as for the full data set to save time. The fitted model is used to predict the CPUE for the left out fold, and the root mean square error is calculated as:</p>
<p><span class="math display">\[RMSE =\frac{1}{n}\sum_{i=1}^{n}(\hat{y_i}-y_i)^2\]</span> where n is the number of observed trips and y is the CPUE data in the left-out tenth of the observer data, and <span class="math inline">\(\hat{y}\)</span> is the CPUE predicted from the model fitted to the other 9/10th of the observer data. The model with the lowest mean RMSE across the 10 folds is selected as the best model. Mean error (ME) is also calculated as an indicator of whether the model has any systematic bias.</p>
<p><span class="math display">\[ME =\frac{1}{n}\sum_{i=1}^{n}\hat{y_i}-y_i\]</span></p>
</div>
<div class="section level4">
<h4 id="final-results-markdown">Final results markdown<a class="anchor" aria-label="anchor" href="#final-results-markdown"></a>
</h4>
<p>The last section of the code saves all the results to a file, and uses markdown to make output files that print all the graphics and tables.</p>
<p>The R markdown file prints a .pdf file with the figures and tables to the output directory for each species labeled with the species and disposition code (e.g. SimulatedSpeciesDeadDiscardResults.pdf). These results may be all that is needed. However, if you want to look more closely at a specific model result, whether or not it was selected by the information criteria and cross validation, all the outputs are printed to .csv files in the folders listed for each species. The total bycatch figures for each individual model show both the analytical estimate of the total bycatch, and the mean across the Monte Carlo draws. These should be very similar, but may be different in years with small sample sizes, due to sampling error.</p>
</div>
</div>
<div class="section level2">
<h2 id="conclusions">Conclusions<a class="anchor" aria-label="anchor" href="#conclusions"></a>
</h2>
<p>Although this code automates much of the process of model selection, it is not recommended that the final model be used without looking closely at the outputs. The selected model must have good fits and should be reasonably consistent with data according to the DHARMa residuals. The results should appear reasonable and be around the same scale as the ratio estimator results. The model should not be overly complex. Also, look at the model selection table to see if other models are also supported by the data.</p>
</div>
<div class="section level2 unnumbered">
<h2 id="references">References<a class="anchor" aria-label="anchor" href="#references"></a>
</h2>
<div id="refs" class="references">
<div id="ref-gridExtra">
<p>Auguie, Baptiste. 2017. <em>GridExtra: Miscellaneous Functions for "Grid" Graphics</em>. <a href="https://CRAN.R-project.org/package=gridExtra" class="external-link">https://CRAN.R-project.org/package=gridExtra</a>.</p>
</div>
<div id="ref-MuMIn">
<p>Barton, Kamil. 2020. <em>MuMIn: Multi-Model Inference</em>. <a href="https://CRAN.R-project.org/package=MuMIn" class="external-link">https://CRAN.R-project.org/package=MuMIn</a>.</p>
</div>
<div id="ref-lme4">
<p>Bates, Douglas, Martin Mächler, Ben Bolker, and Steve Walker. 2015. “Fitting Linear Mixed-Effects Models Using lme4.” <em>Journal of Statistical Software</em> 67 (1): 1–48. <a href="https://doi.org/10.18637/jss.v067.i01" class="external-link">https://doi.org/10.18637/jss.v067.i01</a>.</p>
</div>
<div id="ref-glmmTMB">
<p>Brooks, Mollie E., Kasper Kristensen, Koen J. van Benthem, Arni Magnusson, Casper W. Berg, Anders Nielsen, Hans J. Skaug, Martin Maechler, and Benjamin M. Bolker. 2017. “glmmTMB Balances Speed and Flexibility Among Packages for Zero-Inflated Generalized Linear Mixed Modeling.” <em>The R Journal</em> 9 (2): 378–400. <a href="https://journal.r-project.org/archive/2017/RJ-2017-066/index.html" class="external-link">https://journal.r-project.org/archive/2017/RJ-2017-066/index.html</a>.</p>
</div>
<div id="ref-doParallel">
<p>Corporation, Microsoft, and Steve Weston. 2020. <em>DoParallel: Foreach Parallel Adaptor for the ’Parallel’ Package</em>. <a href="https://CRAN.R-project.org/package=doParallel" class="external-link">https://CRAN.R-project.org/package=doParallel</a>.</p>
</div>
<div id="ref-tweedie">
<p>Dunn, Peter K., and Gordon K. Smyth. 2005. “Series Evaluation of Tweedie Exponential Dispersion Models.” <em>Statistics and Computing</em> 15: 267–80.</p>
</div>
<div id="ref-DHARMa">
<p>Hartig, Florian. 2020. <em>DHARMa: Residual Diagnostics for Hierarchical (Multi-Level / Mixed) Regression Models</em>. <a href="https://CRAN.R-project.org/package=DHARMa" class="external-link">https://CRAN.R-project.org/package=DHARMa</a>.</p>
</div>
<div id="ref-tidyselect">
<p>Henry, Lionel, and Hadley Wickham. 2020. <em>Tidyselect: Select from a Set of Strings</em>. <a href="https://CRAN.R-project.org/package=tidyselect" class="external-link">https://CRAN.R-project.org/package=tidyselect</a>.</p>
</div>
<div id="ref-gt">
<p>Iannone, Richard, Joe Cheng, and Barret Schloerke. 2020. <em>Gt: Easily Create Presentation-Ready Display Tables</em>. <a href="https://CRAN.R-project.org/package=gt" class="external-link">https://CRAN.R-project.org/package=gt</a>.</p>
</div>
<div id="ref-quantreg">
<p>Koenker, Roger. 2021. <em>Quantreg: Quantile Regression</em>. <a href="https://CRAN.R-project.org/package=quantreg" class="external-link">https://CRAN.R-project.org/package=quantreg</a>.</p>
</div>
<div id="ref-Lo">
<p>Lo, N. C. H., L. D. Jacobson, and J. L. Squire. 1992. “Indexes of Relative Abundance from Fish Spotter Data Based on Delta-Lognormal Models.” Journal Article. <em>Canadian Journal of Fisheries and Aquatic Sciences</em> 49 (12): 2515–26. <a href="https://doi.org/Doi%2010.1139/F92-278" class="external-link">https://doi.org/Doi 10.1139/F92-278</a>.</p>
</div>
<div id="ref-foreach">
<p>Microsoft, and Steve Weston. 2020. <em>Foreach: Provides Foreach Looping Construct</em>. <a href="https://CRAN.R-project.org/package=foreach" class="external-link">https://CRAN.R-project.org/package=foreach</a>.</p>
</div>
<div id="ref-pdftools">
<p>Ooms, Jeroen. 2020. <em>Pdftools: Text Extraction, Rendering and Converting of Pdf Documents</em>. <a href="https://CRAN.R-project.org/package=pdftools" class="external-link">https://CRAN.R-project.org/package=pdftools</a>.</p>
</div>
<div id="ref-R-base">
<p>R Core Team. 2020. <em>R: A Language and Environment for Statistical Computing</em>. Vienna, Austria: R Foundation for Statistical Computing. <a href="https://www.R-project.org/" class="external-link">https://www.R-project.org/</a>.</p>
</div>
<div id="ref-RStudio">
<p>RStudio Team. 2020. <em>RStudio: Integrated Development Environment for R</em>. Boston, MA: RStudio, PBC. <a href="http://www.rstudio.com/" class="external-link">http://www.rstudio.com/</a>.</p>
</div>
<div id="ref-MASS">
<p>Venables, W. N., and B. D. Ripley. 2002. <em>Modern Applied Statistics with S</em>. Fourth. New York: Springer. <a href="http://www.stats.ox.ac.uk/pub/MASS4" class="external-link">http://www.stats.ox.ac.uk/pub/MASS4</a>.</p>
</div>
<div id="ref-reshape2">
<p>Wickham, Hadley. 2007. “Reshaping Data with the reshape Package.” <em>Journal of Statistical Software</em> 21 (12): 1–20. <a href="http://www.jstatsoft.org/v21/i12/" class="external-link">http://www.jstatsoft.org/v21/i12/</a>.</p>
</div>
<div id="ref-ggplot2">
<p>———. 2016. <em>Ggplot2: Elegant Graphics for Data Analysis</em>. Springer-Verlag New York. <a href="https://ggplot2.tidyverse.org" class="external-link">https://ggplot2.tidyverse.org</a>.</p>
</div>
<div id="ref-tidyverse">
<p>Wickham, Hadley, Mara Averick, Jennifer Bryan, Winston Chang, Lucy D’Agostino McGowan, Romain François, Garrett Grolemund, et al. 2019. “Welcome to the tidyverse.” <em>Journal of Open Source Software</em> 4 (43): 1686. <a href="https://doi.org/10.21105/joss.01686" class="external-link">https://doi.org/10.21105/joss.01686</a>.</p>
</div>
<div id="ref-gtable">
<p>Wickham, Hadley, and Thomas Lin Pedersen. 2019. <em>Gtable: Arrange ’Grobs’ in Tables</em>. <a href="https://CRAN.R-project.org/package=gtable" class="external-link">https://CRAN.R-project.org/package=gtable</a>.</p>
</div>
<div id="ref-knitr">
<p>Xie, Yihui. 2015. <em>Dynamic Documents with R and Knitr</em>. Second. Chapman; Hall/CRC.</p>
</div>
<div id="ref-TinyTeX">
<p>———. 2019. “TinyTeX: A Lightweight, Cross-Platform, and Easy-to-Maintain Latex Distribution Based on Tex Live.” <em>TUGboat</em>, no. 1: 30–32. <a href="http://tug.org/TUGboat/Contents/contents40-1.html" class="external-link">http://tug.org/TUGboat/Contents/contents40-1.html</a>.</p>
</div>
<div id="ref-tinytex">
<p>———. 2021. <em>Tinytex: Helper Functions to Install and Maintain Tex Live, and Compile Latex Documents</em>. <a href="https://github.com/yihui/tinytex" class="external-link">https://github.com/yihui/tinytex</a>.</p>
</div>
<div id="ref-cplm">
<p>Zhang, Yanwei. 2013. “Likelihood-Based and Bayesian Methods for Tweedie Compound Poisson Linear Mixed Models.”</p>
</div>
</div>
</div>
  </main><aside class="col-md-3"><nav id="toc"><h2>On this page</h2>
    </nav></aside>
</div>



    <footer><div class="pkgdown-footer-left">
  <p></p>
<p>Developed by Elizabeth Babcock.</p>
</div>

<div class="pkgdown-footer-right">
  <p></p>
<p>Site built with <a href="https://pkgdown.r-lib.org/" class="external-link">pkgdown</a> 2.0.6.</p>
</div>

    </footer>
</div>

  

  

  </body>
</html>
